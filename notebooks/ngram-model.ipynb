{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning a Predictive N-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import oldp_client \n",
    "\n",
    "conf = oldp_client.Configuration()\n",
    "conf.api_key['api_key'] = '123abc'  # Replace this with your API key\n",
    "api_client = oldp_client.ApiClient(conf)\n",
    "cases_api = oldp_client.CasesApi(api_client)\n",
    "cases = cases_api.cases_list(court_id=2).results[0:10]  # court=Europäischer Gerichtshof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import preprocessing\n",
    "\n",
    "def clean(content):\n",
    "    content = preprocessing.remove_pattern(content, r'<br.*>|\\n|\\t', replace_with=' ')\n",
    "    content = preprocessing.remove_pattern(content, r'<[^>]+>')\n",
    "    content = preprocessing.replace_html_special_ents(content)\n",
    "    content = preprocessing.remove_whitespace(content)\n",
    "    return content\n",
    "\n",
    "text = ''\n",
    "for case in cases[0:9]:\n",
    "    text += clean(case.content)\n",
    "    \n",
    "test_text = clean(cases[9].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "\n",
    "class Corpus:\n",
    "\n",
    "    def __init__(self, text):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        self.doc = nlp(text)\n",
    "\n",
    "    def get_words(self):\n",
    "        for token in self.doc:\n",
    "            yield token.text\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        for sent in self.doc.sents:\n",
    "            yield sent\n",
    "                \n",
    "    def get_ngrams(self, n):\n",
    "        for sent in self.get_sentences():\n",
    "            if len(sent) < 10:\n",
    "                continue\n",
    "            for pos in range(len(sent)):\n",
    "                if len(sent)-pos < n:\n",
    "                    break\n",
    "                yield (*[sent[pos+i].text for i in range(n)],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_most_common(n):\n",
    "    counter = collections.Counter(corpus.get_ngrams(n))\n",
    "    print('\\nThe most common {}-grams:'.format(n))\n",
    "    for k, v in counter.most_common(5):\n",
    "        print('{}: {}'.format(k, v))\n",
    "\n",
    "corpus = Corpus(text)\n",
    "\n",
    "print('Number of words in corpus: ', len(list(corpus.get_words())))\n",
    "print('Number of sentences in corpus: ', len(list(corpus.get_sentences())))\n",
    "print('Size of alphabet:', len(set(corpus.get_words())))\n",
    "    \n",
    "print_most_common(1)\n",
    "print_most_common(3)\n",
    "print_most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NgramModel:\n",
    "    \n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        self.ngrams = None\n",
    "        self.alphabet = None\n",
    "    \n",
    "    def learn(self, corpus):\n",
    "        self.ngrams = collections.Counter(corpus.get_ngrams(self.n))\n",
    "        self.alphabet = set(corpus.get_words())\n",
    "        \n",
    "    def predict(self, context):\n",
    "        context = tuple(context.split( ))\n",
    "        if len(context) < self.n - 1:\n",
    "            raise ValueError('The context has to be at least of length {}!'.format(self.n - 1))\n",
    "        if len(context) >= self.n:\n",
    "            context = context[-self.n + 1:]\n",
    "            \n",
    "        matches = {}\n",
    "        for word in self.alphabet:\n",
    "            count = self.ngrams[context + (word,)]\n",
    "            if count > 0:\n",
    "                matches[word] = count\n",
    "        total_count = sum(matches.values(), 0.0)\n",
    "        return {k: v / total_count for k, v in matches.items()}\n",
    "\n",
    "corpus = Corpus(text)\n",
    "\n",
    "model = NgramModel(n=3)\n",
    "model.learn(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict('der Europäischen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval(n):\n",
    "    train_corpus = Corpus(text)\n",
    "    test_corpus = Corpus(test_text)\n",
    "\n",
    "    model = NgramModel(n=n)\n",
    "    model.learn(train_corpus)\n",
    "    \n",
    "    print('\\nN={}:'.format(n))\n",
    "    print('Training cross ent:', cross_ent(model, train_corpus, n))\n",
    "    print('Test cross ent:', cross_ent(model, test_corpus, n))\n",
    "\n",
    "def cross_ent(model, corpus, n):\n",
    "    cross_ent = 0.0\n",
    "    count = 0\n",
    "    for ngram in corpus.get_ngrams(n):\n",
    "        context = ' '.join(ngram[0:n-1])\n",
    "        pred = ngram[n-1]\n",
    "        distr = model.predict(context)\n",
    "\n",
    "        # only count ngrams that occurred in the training data\n",
    "        if pred in distr:\n",
    "            cross_ent -= np.log2(distr[pred])\n",
    "            count += 1\n",
    "        \n",
    "    cross_ent /= count\n",
    "    return cross_ent\n",
    "\n",
    "eval(2)\n",
    "eval(3)\n",
    "eval(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldp-notebook",
   "language": "python",
   "name": "oldp-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
