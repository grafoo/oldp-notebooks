{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning a Predictive N-Gram Model\n",
    "\n",
    "This notebook demonstrates how to use a Markov model to predict the next word in a text of the legal domain. Specifically, we model the language used in [German cases](https://de.wikipedia.org/wiki/Urteil_(Recht)). The focus lies on showing how data from the [Open Legal Data Project](https://openlegaldata.io) can be used to do machine learning.\n",
    "\n",
    "_Note_: This demo is not about building the best predictive model for the legal domain, and not about building a competitive n-gram implementation. We use a simple fixed-order n-gram implementation without escaping, smoothing or exclusion techniques.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install all repo requirements by running:\n",
    "```\n",
    "pipenv --python 3.7\n",
    "pipenv install\n",
    "```\n",
    "\n",
    "To install this environment as a Jupyter Notebook kernel run:\n",
    "```\n",
    "pipenv run python -m ipykernel install --name oldp-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain\n",
    "\n",
    "We obtain the training (and test) data using the [OLDP SDK for Python](https://github.com/openlegaldata/oldp-sdk-python). For a more detailed example about the API client usage refer to the [OLDP Client Demo](https://github.com/openlegaldata/oldp-notebooks/blob/master/notebooks/oldp-client-demo.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oldp_client \n",
    "\n",
    "conf = oldp_client.Configuration()\n",
    "conf.api_key['api_key'] = '123abc'  # Replace this with your API key\n",
    "api_client = oldp_client.ApiClient(conf)\n",
    "cases_api = oldp_client.CasesApi(api_client)\n",
    "cases = cases_api.cases_list(court_id=2).results  # first page (10 cases) for court=Europäischer Gerichtshof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "\n",
    "The raw data that we obtain from the API is in the HTML format. Before we can tokenize the text we have to clean it from the HTML tags and some special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ...<h2>Tenor</h2>\n",
      "\n",
      "<div>\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t<p>Als funktional zust&#228;ndig wird die allgemeine Zivilkammer be...\n",
      "After: ...Tenor Als funktional zuständig wird die allgemeine Zivilkammer bestimmt. Gründe I. Die in München an...\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocessing\n",
    "\n",
    "def clean(content):\n",
    "    content = preprocessing.remove_pattern(content, r'\\n|\\t', replace_with=' ')\n",
    "    content = preprocessing.remove_pattern(content, r'<[^>]+>')\n",
    "    content = preprocessing.replace_html_special_ents(content)\n",
    "    content = preprocessing.remove_whitespace(content)\n",
    "    return content\n",
    "\n",
    "text = ''\n",
    "for case in cases:\n",
    "    text += clean(case.content)\n",
    "    \n",
    "print(\"Before: ...{}...\".format(cases[0].content[0:100]))\n",
    "print(\"After: ...{}...\".format(text[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class Corpus:\n",
    "\n",
    "    def __init__(self, text, test_percentage=0.1):\n",
    "        self.test_percentage = test_percentage\n",
    "        \n",
    "        # use spacy NLP to do the tokenization and sentence boundary detection\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        self.doc = nlp(text)\n",
    "\n",
    "    def get_words(self):\n",
    "        for token in self.doc:\n",
    "            yield token.text\n",
    "    \n",
    "    def get_sentences(self, test=False):\n",
    "        for sent in self.doc.sents:\n",
    "            # split into training and test sentences, according to the given percentage\n",
    "            if (np.random.random() >= self.test_percentage and not test) or \\\n",
    "                (np.random.random() < self.test_percentage and test):\n",
    "                yield sent\n",
    "                \n",
    "    def get_ngrams(self, n, test=False):\n",
    "        for sent in self.get_sentences(test=test):\n",
    "            if len(sent) < 10:\n",
    "                continue\n",
    "            for pos in range(len(sent)):\n",
    "                if len(sent)-pos < n:\n",
    "                    break\n",
    "                yield (*[sent[pos+i].text for i in range(n)],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus:  30282\n",
      "Number of training sentences in corpus:  1673\n",
      "Number of test sentences in corpus:  192\n",
      "Size of alphabet: 5270\n",
      "\n",
      "The most common 1-grams:\n",
      "(',',): 1225\n",
      "('.',): 1008\n",
      "('der',): 882\n",
      "('die',): 666\n",
      "('des',): 407\n",
      "\n",
      "The most common 3-grams:\n",
      "(',', 'dass', 'die'): 35\n",
      "('Abs.', '1', 'Satz'): 32\n",
      "(',', 'dass', 'der'): 22\n",
      "('1', 'Satz', '1'): 21\n",
      "('§', '11', 'Abs.'): 18\n",
      "\n",
      "The most common 5-grams:\n",
      "('§', '124', 'Abs.', '2', 'Nr.'): 13\n",
      "('Abs.', '5', 'Satz', '1', 'VwGO'): 8\n",
      "('§', '11', 'Abs.', '2a', 'TierSchG'): 8\n",
      "(',', 'juris', ',', 'Rn', '.'): 7\n",
      "('vom', '19', '.', 'März', '2018'): 7\n"
     ]
    }
   ],
   "source": [
    "def print_most_common(n):\n",
    "    counter = collections.Counter(corpus.get_ngrams(n))\n",
    "    print('\\nThe most common {}-grams:'.format(n))\n",
    "    for k, v in counter.most_common(5):\n",
    "        print('{}: {}'.format(k, v))\n",
    "\n",
    "corpus = Corpus(text)\n",
    "\n",
    "print('Number of words in corpus: ', len(list(corpus.get_words())))\n",
    "print('Number of training sentences in corpus: ', len(list(corpus.get_sentences())))\n",
    "print('Number of test sentences in corpus: ', len(list(corpus.get_sentences(test=True))))\n",
    "print('Size of alphabet:', len(set(corpus.get_words())))\n",
    "    \n",
    "print_most_common(1)\n",
    "print_most_common(3)\n",
    "print_most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Union': 0.75, 'Gemeinschaft': 0.125, 'Kommission': 0.125}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NgramModel:\n",
    "    \n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        self.ngrams = None\n",
    "        self.alphabet = None\n",
    "    \n",
    "    def learn(self, corpus):\n",
    "        self.ngrams = collections.Counter(corpus.get_ngrams(self.n))\n",
    "        self.alphabet = set(corpus.get_words())\n",
    "        \n",
    "    def predict(self, context):    \n",
    "        if len(context) < self.n - 1:\n",
    "            raise ValueError('The context has to be at least of length {}!'.format(self.n - 1))\n",
    "        if len(context) >= self.n:\n",
    "            context = context[-self.n + 1:]\n",
    "            \n",
    "        matches = {}\n",
    "        for word in self.alphabet:\n",
    "            count = self.ngrams[tuple(context) + (word,)]\n",
    "            if count > 0:\n",
    "                matches[word] = count\n",
    "        total_count = sum(matches.values(), 0.0)\n",
    "        return {k: v / total_count for k, v in matches.items()}\n",
    "    \n",
    "    def predict_str(self, context_str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        context = [token.text for token in nlp(context_str)]\n",
    "        return self.predict(context)\n",
    "        \n",
    "\n",
    "corpus = Corpus(text)\n",
    "\n",
    "model = NgramModel(n=3)\n",
    "model.learn(corpus)\n",
    "\n",
    "model.predict(['der', 'Europäischen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret\n",
    "\n",
    "We can use the predictive model to guess the next word in a sentence with legal content. This could be used as an autocompletion feature in a legal text editor.\n",
    "\n",
    "To compare the performance of several fixed-order models, we use cross entropy as a measure. We see that out of the tested values, n=10 has the best test performance. However, presumably due to the training dataset being too small, only about 12% of the contexts could be completed (if a context was not seen in the training data the implemented algorithm does not make a prediction). It seems likely, that the good performance especially with higher _n_ is caused by a large amount of set phrases (or tokens) in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Folgenden'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = model.predict_str('Am 23. Dezember 2006 nahm der Sicherheitsrat der Vereinten Nationen (im')\n",
    "pred_next_word = max(d.keys(), key=lambda key: d[key])\n",
    "pred_next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N=2:\n",
      "Training cross ent: 3.6120551673497037 (count=23229)\n",
      "Test cross ent: 3.5750781588445535 (count=2215)\n",
      "\n",
      "N=3:\n",
      "Training cross ent: 0.8136898275785924 (count=21265)\n",
      "Test cross ent: 0.7985905308149392 (count=2364)\n",
      "\n",
      "N=5:\n",
      "Training cross ent: 0.07329458669651902 (count=19292)\n",
      "Test cross ent: 0.08784666850808225 (count=2149)\n",
      "\n",
      "N=10:\n",
      "Training cross ent: 0.005888186456617001 (count=14754)\n",
      "Test cross ent: 0.0020242914979757085 (count=1482)\n"
     ]
    }
   ],
   "source": [
    "def eval(n):\n",
    "    corpus = Corpus(text)\n",
    "\n",
    "    model = NgramModel(n=n)\n",
    "    model.learn(corpus)\n",
    "    \n",
    "    print('\\nN={}:'.format(n))\n",
    "    print('Training cross ent: {} (count={})'.format(*cross_ent(model, corpus, n)))\n",
    "    print('Test cross ent: {} (count={})'.format(*cross_ent(model, corpus, n, test=True)))\n",
    "\n",
    "def cross_ent(model, corpus, n, test=False):\n",
    "    cross_ent = 0.0\n",
    "    count = 0\n",
    "    for ngram in corpus.get_ngrams(n, test=test):\n",
    "        context = ngram[0:n-1]\n",
    "        pred = ngram[n-1]\n",
    "        distr = model.predict(context)\n",
    "\n",
    "        # only count ngrams that occurred in the training data\n",
    "        if pred in distr:\n",
    "            cross_ent -= np.log2(distr[pred])\n",
    "            count += 1\n",
    "        \n",
    "    cross_ent /= count\n",
    "    return cross_ent, count\n",
    "\n",
    "eval(2)\n",
    "eval(3)\n",
    "eval(5)\n",
    "eval(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldp-notebook",
   "language": "python",
   "name": "oldp-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
